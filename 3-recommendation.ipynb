{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Recommender System\n",
    "\n",
    "In this notebook we will see a simple implementation of a Recommender System based on *Collaborative Filtering*. We will use the MovieLens dataset:\n",
    "\n",
    "https://grouplens.org/datasets/movielens/\n",
    "\n",
    "Since we will store the full utility matrix, we consider a small dataset, and in particular the one recommended for education and development (small version). Such a dataset contains approximately 100,000 ratings to 9,000 films made by 600 users.\n",
    "\n",
    "Users and items (hereinafter, we will use the term \"item\" instead of \"film\") are identified by integers, and the rating is a number bewteen 1 and 5. A sample of the file, which contains as also the timestamps, is:\n",
    "\n",
    "```text\n",
    "userId,movieId,rating,timestamp\n",
    "1,1,4.0,964982703\n",
    "1,3,4.0,964981247\n",
    "1,6,4.0,964982224\n",
    "1,47,5.0,964983815\n",
    "1,50,5.0,964982931\n",
    "1,70,3.0,964982400\n",
    "1,101,5.0,964980868\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data\n",
    "\n",
    "\n",
    "We first define the function to load the data. As usual, we need to adapt such a function to the specific file input format.\n",
    "\n",
    "In particular, we are going to assign to users and items progressive numbers, so their identifications will be also the indexes of the utility matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename: str) -> tuple[list[list[float]], int, int]:\n",
    "    \"\"\"Loads and processes rating data from a CSV file into indexed format.\n",
    "\n",
    "    Args:\n",
    "        filename: Path to the input CSV file. The file should contain\n",
    "            rows in the format: user_id, item_id, rating (with a header row).\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing:\n",
    "            - A list of [user_idx, item_idx, rating] entries.\n",
    "            - Total number of unique users.\n",
    "            - Total number of unique items.\n",
    "    \"\"\"\n",
    "    input_lines = []\n",
    "    users = {}\n",
    "    num_users = 0\n",
    "    items = {}\n",
    "    num_items = 0\n",
    "    raw_lines = open(filename, \"r\").read().splitlines()\n",
    "    # Remove the first line\n",
    "    del raw_lines[0]\n",
    "    for line in raw_lines:\n",
    "        line_content = line.split(\",\")\n",
    "        user_id = int(line_content[0])\n",
    "        item_id = int(line_content[1])\n",
    "        rating = float(line_content[2])\n",
    "        if user_id not in users:\n",
    "            users[user_id] = num_users\n",
    "            num_users += 1\n",
    "        if item_id not in items:\n",
    "            items[item_id] = num_items\n",
    "            num_items += 1\n",
    "        input_lines.append([users[user_id], items[item_id], rating])\n",
    "    return input_lines, num_users, num_items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why do we use the following if statements?\n",
    "\n",
    "```python\n",
    "        if user_id not in users:\n",
    "            users[user_id] = num_users\n",
    "            num_users += 1\n",
    "        if item_id not in items:\n",
    "            items[item_id] = num_items\n",
    "            num_items += 1\n",
    "```\n",
    "\n",
    "Let's explain it with an example:\n",
    "\n",
    "| userId | movieId | rating |\n",
    "| :----: | :-----: | :----: |\n",
    "|   10   |   100   |   4.0  |\n",
    "|   10   |   300   |   5.0  |\n",
    "|   25   |   100   |   3.0  |\n",
    "\n",
    "1. Process the first line (10,100,4.0):\n",
    "\n",
    "- `user_id = 10` is new ⇒ assign `users[10] = 0` and increment `num_users` to 1.\n",
    "\n",
    "- `item_id = 100` is new ⇒ assign `items[100] = 0` and increment `num_items` to 1.\n",
    "\n",
    "Append `[ users[10], items[100], 4.0 ]` ⇒ `[0, 0, 4.0]`\n",
    "\n",
    "2. Process the second line (10,300,5.0):\n",
    "\n",
    "- `user_id = 10` already seen ⇒ `users[10]` stays 0.\n",
    "\n",
    "- `item_id = 300` is new ⇒ assign `items[300] = 1` and increment `num_items` to 2.\n",
    "\n",
    "Append `[0, 1, 5.0]`\n",
    "\n",
    "And so on, until we obtain:\n",
    "\n",
    "```python\n",
    "users  = {10: 0, 25: 1}\n",
    "items  = {100: 0, 300: 1}\n",
    "input_lines = [\n",
    "  [0, 0, 4.0],\n",
    "  [0, 1, 5.0],\n",
    "  [1, 0, 3.0]\n",
    "]\n",
    "num_users = 2\n",
    "num_items = 2\n",
    "```\n",
    "\n",
    "**Note**: We remap the original `user_id` and `item_id` to consecutive zero-based integers (e.g., from 10 to 0, and from 25 to 1). This makes the data easier to store in dense matrix or tensor formats, speeds up computation, and avoids dealing with sparse or large original IDs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input file containing the dataset is called \"3-ratings.csv\".\n",
    "\n",
    "On Colab, remember to mount your Drive\n",
    "```python\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "input_file = \"/content/drive/My Drive/...\"\n",
    "```\n",
    "Let's load our dataset and see its initial content:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = \"./data/3-ratings.csv\"\n",
    "\n",
    "input_ratings, num_users, num_items = load_data(input_file)\n",
    "\n",
    "print(\n",
    "    \"\\nThe dataset contains\",\n",
    "    num_users,\n",
    "    \"users,\",\n",
    "    num_items,\n",
    "    \"items, and\",\n",
    "    len(input_ratings),\n",
    "    \"ratings.\\n\",\n",
    ")\n",
    "print(\"The first five ratings are:\", input_ratings[:5], \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility matrix\n",
    "\n",
    "Building and maintaining in memory the utility matrix is inefficient, since the matrix is sparse. But working with a matrix is more intuitive, therefore we will use such an approach. \n",
    "\n",
    "The best way to handle a matrix and the operations associated to it is to use Numpy arrays. If you are not familiar with Numpy, you can find different tutorials online. See for instance:\n",
    "\n",
    "https://www.kaggle.com/saptarsi/numpy-tutorial-notebook-sg  \n",
    "https://cs231n.github.io/python-numpy-tutorial/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def create_utility_matrix(\n",
    "    input_ratings: list[list[float]], num_users: int, num_items: int\n",
    ") -> np.ndarray:\n",
    "    # In the utils.py file we log the sparsity\n",
    "    # While here we print it for simplicity\n",
    "    \"\"\"Builds the user-item utility matrix and logs its sparsity.\n",
    "\n",
    "    This function is designed to work directly with the output of `load_data()`.\n",
    "    It takes the remapped user/item indices and constructs a dense matrix where\n",
    "    each cell [i, j] contains the rating assigned by user i to item j.\n",
    "\n",
    "    Args:\n",
    "        input_ratings: List of [user_idx, item_idx, rating] triples,\n",
    "            as returned by `load_data()`.\n",
    "        num_users: Total number of users (also from `load_data()`).\n",
    "        num_items: Total number of items (also from `load_data()`).\n",
    "\n",
    "    Returns:\n",
    "        A 2D NumPy array representing the utility matrix.\n",
    "    \"\"\"\n",
    "    # Create an NxI matrix of zeros,\n",
    "    # where N = number of users\n",
    "    # and I = number of items\n",
    "    ratings = np.zeros((num_users, num_items))\n",
    "\n",
    "    # Fill the matrix with the ratings\n",
    "    # NOTE: input_ratings: list[list]\n",
    "    for row in input_ratings:\n",
    "        ratings[int(row[0]), int(row[1])] = row[2]\n",
    "\n",
    "    # Compute the \"sparsity\", i.e., percentage of non-zero cells\n",
    "    sparsity = 100 * float(np.count_nonzero(ratings)) / float(num_users * num_items)\n",
    "    print(\"Sparsity: %.2f%%\" % sparsity)\n",
    "\n",
    "    return ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = create_utility_matrix(input_ratings, num_users, num_items)\n",
    "\n",
    "ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.70% sparsity means:\n",
    "- Only 1.7% of user–item pairs have known ratings.\n",
    "- 98.3% of the matrix is unknown → needs to be predicted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary analysis\n",
    "\n",
    "In the following, we suggest a set of preliminary analysis on the dataset that can be carried out with simple operations on the matrix:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question  Q1\n",
    "<div class=\"alert alert-info\">\n",
    "For a given user id, find the number of rated items by that user, and the average rating.  \n",
    "    \n",
    "- **Hint**: Compute these values once for all users, and store them in another Nx2 matrix.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer here\n",
    "def count_and_avg_rating_per_user(utility_matrix: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Computes the number of rated items and average rating per user.\n",
    "\n",
    "    Args:\n",
    "        utility_matrix: A 2D NumPy array of shape (num_users, num_items),\n",
    "            where each cell [i, j] contains the rating from user i for item j,\n",
    "            or 0 if no rating was given.\n",
    "\n",
    "    Returns:\n",
    "        A NumPy array of shape (num_users, 2), where:\n",
    "            - [:, 0] contains the count of rated items per user.\n",
    "            - [:, 1] contains the average rating per user (0 if no ratings).\n",
    "    \"\"\"\n",
    "    counts = np.count_nonzero(utility_matrix, axis=1)\n",
    "    sums = utility_matrix.sum(axis=1)\n",
    "    averages = np.zeros_like(sums, dtype=float)\n",
    "    mask = counts > 0\n",
    "    averages[mask] = sums[mask] / counts[mask]\n",
    "    return np.vstack((counts, averages)).T\n",
    "\n",
    "\n",
    "users_info = count_and_avg_rating_per_user(ratings)\n",
    "users_info[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question  Q2\n",
    "<div class=\"alert alert-info\">\n",
    "Find the top-k viewers, i.e., users that rated the highest number of items.\n",
    "    \n",
    "- *Variation*: find the bottom-k viewers.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer here\n",
    "def top_bottom_users_by_count(\n",
    "    user_info: np.ndarray, k: int, bottom_k: bool = False\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Returns the top or bottom k users based on the number of rated items, including their indices.\n",
    "\n",
    "    By default, the function returns the top k users with the highest number of rated items.\n",
    "    If `bottom_k` is True, it returns the bottom k users with the lowest number of rated items.\n",
    "\n",
    "    Args:\n",
    "        user_info (np.ndarray): NumPy array of shape (num_users, 2), where:\n",
    "            - [:, 0] contains the count of rated items per user.\n",
    "            - [:, 1] contains the average rating per user.\n",
    "        k (int): Number of users to return.\n",
    "        bottom_k (bool, optional): If True, returns the bottom k users instead of the top. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Array of shape (k, 3), where each row contains:\n",
    "            [user_index, count, average_rating].\n",
    "    \"\"\"\n",
    "    if not bottom_k:\n",
    "        indices = np.argsort(user_info[:, 0])[::-1][:k]\n",
    "    else:\n",
    "        indices = np.argsort(user_info[:, 0])[:k]\n",
    "\n",
    "    return np.column_stack((indices, user_info[indices]))\n",
    "\n",
    "\n",
    "top_10_users_by_count = top_bottom_users_by_count(users_info, 10)\n",
    "\n",
    "# Print results without scientific notation\n",
    "np.set_printoptions(suppress=True)\n",
    "print(top_10_users_by_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question  Q3\n",
    "<div class=\"alert alert-info\">\n",
    "For a given item id, find the number of users that rated it, and its average rating.\n",
    "    \n",
    "- **Hint**: Compute these values once for all items, and store them in another Ix2 matrix.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer here\n",
    "def count_and_avg_ratings_per_item(utility_matrix: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Computes the number of ratings and average rating per item.\n",
    "\n",
    "    Args:\n",
    "        utility_matrix: A 2D NumPy array of shape (num_users, num_items),\n",
    "            where each cell [i, j] contains the rating from user i for item j,\n",
    "            or 0 if no rating was given.\n",
    "\n",
    "    Returns:\n",
    "        A NumPy array of shape (num_items, 2), where:\n",
    "            - [:, 0] contains the count of ratings per item.\n",
    "            - [:, 1] contains the average rating per item (0 if unrated).\n",
    "    \"\"\"\n",
    "    counts = np.count_nonzero(utility_matrix, axis=0)\n",
    "    sums = utility_matrix.sum(axis=0)\n",
    "    averages = np.zeros_like(sums, dtype=float)\n",
    "    mask = counts > 0\n",
    "    averages[mask] = sums[mask] / counts[mask]\n",
    "    return np.vstack((counts, averages)).T\n",
    "\n",
    "\n",
    "items_info = count_and_avg_ratings_per_item(ratings)\n",
    "items_info[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question  Q4\n",
    "<div class=\"alert alert-info\">\n",
    "Find the top-k items with at least v views, i.e., the k items with the highest average rate that has been rated by at least v users.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer here\n",
    "def top_items_by_avg_and_count(\n",
    "    item_info: np.ndarray, min_count: int, top_k: int\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Returns the top-k items with the highest average rating among those with sufficient ratings.\n",
    "\n",
    "    Args:\n",
    "        item_info: NumPy array of shape (num_items, 2), where:\n",
    "            - [:, 0] contains the count of ratings per item.\n",
    "            - [:, 1] contains the average rating per item.\n",
    "        min_count: Minimum number of ratings required to include an item.\n",
    "        top_k: Number of top items to return (default is 10).\n",
    "\n",
    "    Returns:\n",
    "        A NumPy array of shape (top_k, 3), where each row contains:\n",
    "            [item_index, num_ratings, avg_rating].\n",
    "    \"\"\"\n",
    "    mask = item_info[:, 0] >= min_count\n",
    "    filtered = item_info[mask]\n",
    "    indices = np.where(mask)[0]\n",
    "\n",
    "    sorted_idx = np.argsort(filtered[:, 1])[::-1][:top_k]\n",
    "    return np.column_stack((indices[sorted_idx], filtered[sorted_idx]))\n",
    "\n",
    "\n",
    "top_10_items_with_100_ratings = top_items_by_avg_and_count(\n",
    "    items_info, min_count=100, top_k=10\n",
    ")\n",
    "np.set_printoptions(suppress=True)\n",
    "print(top_10_items_with_100_ratings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question  Q5\n",
    "<div class=\"alert alert-info\">\n",
    "Normalize the utility matrix by subtracting from each non-zero cell at row i the average rating of the user i.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer here\n",
    "def normalize_utility_matrix_by_user(utility_matrix: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Normalizes the utility matrix by subtracting each user's average rating.\n",
    "\n",
    "    Each nonzero entry is replaced with (rating - user_avg). Unrated entries remain 0.\n",
    "\n",
    "    Args:\n",
    "        utility_matrix: A 2D NumPy array of shape (num_users, num_items),\n",
    "            where each cell [i, j] contains the rating from user i for item j,\n",
    "            or 0 if no rating was given.\n",
    "\n",
    "    Returns:\n",
    "        A normalized utility matrix of the same shape as the input,\n",
    "        with zero-mean rows (for nonzero entries).\n",
    "    \"\"\"\n",
    "    user_info = count_and_avg_rating_per_user(utility_matrix)\n",
    "    avg_ratings = user_info[:, 1][:, None]\n",
    "    return np.where(utility_matrix != 0, utility_matrix - avg_ratings, 0)\n",
    "\n",
    "\n",
    "ratings_norm_u = normalize_utility_matrix_by_user(ratings)\n",
    "print(ratings_norm_u[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_utility_matrix_by_item(utility_matrix: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Normalizes the utility matrix by subtracting each item's average rating.\n",
    "\n",
    "    Each nonzero entry is replaced with (rating - item_avg). Unrated entries remain 0.\n",
    "\n",
    "    Args:\n",
    "        utility_matrix: A 2D NumPy array of shape (num_users, num_items),\n",
    "            where each cell [i, j] contains the rating from user i for item j,\n",
    "            or 0 if no rating was given.\n",
    "\n",
    "    Returns:\n",
    "        A normalized utility matrix of the same shape as the input,\n",
    "        with zero-mean columns (for nonzero entries).\n",
    "    \"\"\"\n",
    "    item_info = count_and_avg_ratings_per_item(utility_matrix)\n",
    "    avg_ratings = item_info[:, 1][None, :]\n",
    "    return np.where(utility_matrix != 0, utility_matrix - avg_ratings, 0)\n",
    "\n",
    "\n",
    "ratings_norm_i = normalize_utility_matrix_by_item(ratings)\n",
    "print(ratings_norm_i[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use of `[:, None]`:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "a = np.array([10, 20, 30])\n",
    "print(a.shape)          # (3,)\n",
    "\n",
    "b = a[:, None]\n",
    "print(b.shape)          # (3, 1)\n",
    "print(b)\n",
    "# Output:\n",
    "# [[10]\n",
    "#  [20]\n",
    "#  [30]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation of `np.where(condition, x, y)`\n",
    "\n",
    "```python\n",
    "if condition:\n",
    "    use x\n",
    "else:\n",
    "    use y\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the utility matrix\n",
    "\n",
    "We want to separate the values of utility matrix into two sets, train and test:\n",
    "\n",
    "- The train set is used to compute the similarity between users or items;\n",
    "- Using the similarity, we will then predict the ratings;\n",
    "- We compare the prediction with the values in the test set to compute the prediction error.\n",
    "\n",
    "**Note**: `ratings` simply represents the utility matrix (num_users, num_items). We computed it at the beginning of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## previous definition\n",
    "def train_test_split(\n",
    "    ratings: np.ndarray, sample_per_user: int = 10\n",
    ") -> tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Splits the utility matrix into train and test sets by sampling a fixed number of ratings per user.\n",
    "\n",
    "    For each user, exactly `sample_per_user` rated items are randomly selected and moved to the test set.\n",
    "    The train and test matrices are guaranteed to be disjoint.\n",
    "\n",
    "    Args:\n",
    "        ratings: A 2D NumPy array of shape (num_users, num_items),\n",
    "            containing the full utility matrix with ratings.\n",
    "        sample_per_user: Number of ratings to sample per user for the test set.\n",
    "\n",
    "    Returns:\n",
    "        A tuple of two NumPy arrays (train, test), each of shape (num_users, num_items).\n",
    "    \"\"\"\n",
    "    test = np.zeros(ratings.shape)\n",
    "    train = ratings.copy()\n",
    "    for user in range(ratings.shape[0]):\n",
    "        test_ratings = np.random.choice(\n",
    "            ratings[user, :].nonzero()[0],\n",
    "            size=sample_per_user,\n",
    "            replace=False,\n",
    "        )\n",
    "        train[user, test_ratings] = 0.0\n",
    "        test[user, test_ratings] = ratings[user, test_ratings]\n",
    "\n",
    "    assert np.all((train * test) == 0)\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example above function usage:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "# Utility matrix (2, 5)\n",
    "ratings = np.array([\n",
    "    [5, 0, 3, 0, 4],  # user 0\n",
    "    [0, 2, 0, 0, 1],  # user 1\n",
    "])\n",
    "\n",
    "for user in range(ratings.shape[0]):\n",
    "    test_ratings = np.random.choice(ratings[user, :].nonzero()[0], \n",
    "                                    size=sample_per_user, \n",
    "                                    replace=False)\n",
    "\n",
    "```\n",
    "\n",
    "What happens inside the for loop (**User 0**):\n",
    "- `ratings[0, :]` = `[5, 0, 3, 0, 4]`\n",
    "- `ratings[0, :].nonzero()[0]` → `np.array([0, 2, 4])` → indexes of non-zero ratings\n",
    "- `np.random.choice([0, 2, 4], size=2, replace=False)` → randomly selects 2 indices from `[0, 2, 4]`, e.g., `[2, 4]`\n",
    "\n",
    "User 1:\n",
    "- `ratings[1, :]` = `[0, 2, 0, 0, 1]`\n",
    "- Non-zero indices: `[1, 4]`\n",
    "- `np.random.choice([1, 4], size=2, replace=False)` → `[4, 1]` (**Note**: order doesn’t matter)\n",
    "\n",
    "**Note**: The sampled indices will be zeroed out in train and copied into test.\n",
    "\n",
    "Finally, we use assure that train and test are truly dijoint with the line:\n",
    "```python\n",
    "assert np.all((train * test) == 0)\n",
    "```\n",
    "\n",
    "If any position has a value in both matrices, the product will be nonzero. \n",
    "`np.all(...) == 0` ensures all products are zero, i.e., no overlap anywhere.\n",
    "\n",
    "assert syntax:\n",
    "```python\n",
    "assert condition, \"Optional error message\"\n",
    "```\n",
    "\n",
    "If the condition is:\n",
    "- `True` → nothing happens, code continues.\n",
    "- `False` → an `AssertionError` is raised and execution stops.\n",
    "\n",
    "**Note**: Assertions are mainly used for debugging and internal sanity checks. don’t rely on them for critical validation in production code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split_v2(\n",
    "    ratings: np.ndarray, sample_per_user: int = 10, seed: int = 123425536\n",
    ") -> tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Splits the utility matrix into train and test sets for evaluation.\n",
    "\n",
    "    For each user, a percentage of their ratings is randomly selected and moved to the test set.\n",
    "    The resulting train and test matrices are disjoint.\n",
    "\n",
    "    Args:\n",
    "        ratings: A 2D NumPy array of shape (num_users, num_items),\n",
    "            containing the full utility matrix with ratings.\n",
    "        sample_per_user: Percentage of each user's ratings to sample for the test set.\n",
    "        seed: Random seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "        A tuple of two NumPy arrays (train, test), each of shape (num_users, num_items).\n",
    "    \"\"\"\n",
    "    test = np.zeros(ratings.shape)\n",
    "    train = ratings.copy()\n",
    "    np.random.seed(seed)\n",
    "    for user in range(ratings.shape[0]):\n",
    "        num_ratings = len(ratings[user, :].nonzero()[0])\n",
    "        if num_ratings == 0:\n",
    "            continue\n",
    "        actual_sample = int(num_ratings * sample_per_user / 100)\n",
    "        test_ratings = np.random.choice(\n",
    "            ratings[user, :].nonzero()[0],\n",
    "            size=actual_sample,\n",
    "            replace=False,\n",
    "        )\n",
    "        train[user, test_ratings] = 0.0\n",
    "        test[user, test_ratings] = ratings[user, test_ratings]\n",
    "\n",
    "    assert np.all((train * test) == 0)\n",
    "    return train, test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main differences with the prior `train_test_split` function:\n",
    "1. `sample_per_user` is now interpreted as a percentage, not as an absolute count anymore.\n",
    "```python \n",
    "actual_sample = int(num_ratings * sample_per_user / 100)\n",
    "```\n",
    "2. Random seed added.\n",
    "\n",
    "3. Users with no ratings are skipped safely.\n",
    "```python\n",
    "if num_ratings == 0:\n",
    "    continue\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run the above function by using 15 samples from each user for the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split_v2(ratings, sample_per_user=10)\n",
    "\n",
    "print(\"Non-zero elements in ratings \", np.count_nonzero(ratings))\n",
    "print(\"Non-zero elements in train \", np.count_nonzero(train))\n",
    "print(\"Non-zero elements in test \", np.count_nonzero(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this version can be used after having answered to Q5\n",
    "\n",
    "train_u, test_u = train_test_split_v2(ratings_norm_u, sample_per_user=10)\n",
    "train_i, test_i = train_test_split_v2(ratings_norm_i, sample_per_user=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing the similarity matrix \n",
    "\n",
    "Using the train set, we compute the user-user similarity matrix (NxN) and the item-item similairyt matrix (IxI). From the mathematical point of view, we have for users x and y:\n",
    "\n",
    "$$\n",
    "sim(x,y) = \\cos(r_x, r_y) = \\frac{\\sum_i r_{xi} r_{yi}}{\\sqrt{\\sum_i r_{xi}^2}\\sqrt{\\sum_i r_{yi}^2}}\n",
    "$$\n",
    "\n",
    "A similar computation can be done for the item-item similarity.\n",
    "\n",
    "Each matrix is computed using matrix operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cosine similarity (alternatively, Pearson correlation) is used in memory-based collaborative filtering\n",
    "# (e.g., user-user or item-item CF).\n",
    "# It is not applicable for rGLSVD and sGLSVD, which require clustering methods to assign users into fixed subsets.\n",
    "def compute_similarity(\n",
    "    ratings: np.ndarray,\n",
    "    kind: str = \"user\",\n",
    "    epsilon: float = 1e-9,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Returns a cosine-similarity matrix for users or items.\n",
    "\n",
    "    Args:\n",
    "        ratings: Utility matrix of shape (num_users, num_items), where each\n",
    "            non-zero entry denotes user-item interaction strength.\n",
    "        kind: Either `\"user\"` for a user-user similarity matrix or `\"item\"` for\n",
    "            an item-item similarity matrix.\n",
    "        epsilon: Small constant added to the dot-product matrix to prevent\n",
    "            division-by-zero when normalizing.\n",
    "\n",
    "    Returns:\n",
    "        out:\n",
    "        A square NumPy array:\n",
    "            * (num_users, num_users) if `kind == \"user\"`,\n",
    "            * (num_items, num_items) if `kind == \"item\"`,\n",
    "        where each entry ∈ [0, 1] is the cosine similarity between the\n",
    "        corresponding user or item vectors.\n",
    "    \"\"\"\n",
    "    # We compute the dot product between ratings\n",
    "    # epsilon -> small number for handling divide-by-zero errors\n",
    "    if kind == \"user\":\n",
    "        sim = ratings.dot(ratings.T) + epsilon\n",
    "    elif kind == \"item\":\n",
    "        sim = ratings.T.dot(ratings) + epsilon\n",
    "    else:\n",
    "        raise ValueError(\"kind must be 'user' or 'item'.\")\n",
    "\n",
    "    # From the diagonal of the dot product matrix we extract the norms\n",
    "    # (diagonal contains squared magnitudes: v·v = ||v||²)\n",
    "    norms = np.array([np.sqrt(np.diagonal(sim))])\n",
    "\n",
    "    # The double division allows broadcasting\n",
    "    result: np.ndarray = sim / norms / norms.T\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to compute the two matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_similarity = compute_similarity(train, kind=\"user\")\n",
    "item_similarity = compute_similarity(train, kind=\"item\")\n",
    "\n",
    "# Show the first values of the item-item similairty matrix\n",
    "print(item_similarity[:4, :4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This version can be used after having answered to Q5\n",
    "\n",
    "user_similarity_n = compute_similarity(train_u, kind=\"user\")\n",
    "item_similarity_n = compute_similarity(train_i, kind=\"item\")\n",
    "\n",
    "# Show the first values of the item-item similairty matrix\n",
    "print(item_similarity_n[:4, :4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing the prediction and the prediction error\n",
    "\n",
    "Using the similarity matrix, we predict the rating of missing values. In case of user-user similarity, if we want to predict item i for user x, we have:\n",
    "\n",
    "$$\n",
    "\\hat{r}_{xi} = \\frac{\\sum_y sim(x,y) r_{yi}}{\\sum_y sim(x,y)}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_simple(ratings, similarity, kind=\"user\"):\n",
    "    if kind == \"user\":\n",
    "        return similarity.dot(ratings) / np.array([similarity.sum(axis=1)]).T\n",
    "    elif kind == \"item\":\n",
    "        return ratings.dot(similarity) / np.array([similarity.sum(axis=1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute the error, we use the mean square error from Sklearn library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "def get_mse(pred, actual):\n",
    "    # Ignore nonzero terms.\n",
    "    pred = pred[actual.nonzero()].flatten()\n",
    "    actual = actual[actual.nonzero()].flatten()\n",
    "    return mean_squared_error(pred, actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prediction = predict_simple(train, user_similarity, kind=\"user\")\n",
    "item_prediction = predict_simple(train, item_similarity, kind=\"item\")\n",
    "\n",
    "print(\"User-based CF MSE: \", get_mse(user_prediction, test))\n",
    "print(\"Item-based CF MSE: \", get_mse(item_prediction, test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this version can be used after having answered to Q5\n",
    "\n",
    "user_prediction_n = predict_simple(train_u, user_similarity_n, kind=\"user\")\n",
    "item_prediction_n = predict_simple(train_i, item_similarity_n, kind=\"item\")\n",
    "\n",
    "print(\"User-based CF MSE: \", get_mse(user_prediction_n, test_u))\n",
    "print(\"Item-based CF MSE: \", get_mse(item_prediction_n, test_i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional questions\n",
    "\n",
    "The above procedure computes the similairity and the prediction for all users or all items. It would be interesting to work on single users, and see if we can improve the error for that user.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question  Q6\n",
    "<div class=\"alert alert-info\">\n",
    "Consider the user similarity matrix and the item similarity matrix. \n",
    "For a given user id, consider the ratings in the test set. Predict those ratings using the user-user similarity matrix or with the items-items similairty matrix, and compute the error.\n",
    "    \n",
    "- **Note**: We are not interested in the error computed over all users, but only over for a specific user.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question  Q7\n",
    "<div class=\"alert alert-info\">\n",
    "Repeat the above computation, but consider the error for only the top-5 recommended items.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question  Q8\n",
    "<div class=\"alert alert-info\">\n",
    "Repeat the computations for questions Q5 and Q6, but, as recommendation, consider the top 30 most similar users or items, and check if this has an impact on the error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## rGLSVD Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We start by splitting the utility matrix into train and test data\n",
    "train_u, test_u = train_test_split_v2(ratings, sample_per_user=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import binarize_ratings\n",
    "\n",
    "# Simple function to binarize the utility matrix\n",
    "binarized_train_u = binarize_ratings(train_u)\n",
    "\n",
    "binarized_train_u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Literal\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "\n",
    "def cluster_users_kmeans_grid(\n",
    "    utility_matrix: np.ndarray,\n",
    "    k_values: list[int],\n",
    "    random_state: int = 42,\n",
    "    select_by: Literal[\"silhouette\", \"none\"] = \"silhouette\",\n",
    ") -> tuple[np.ndarray, int, dict[int, float]]:\n",
    "    \"\"\"Cluster users with KMeans over k and (optionally) pick k by silhouette.\n",
    "\n",
    "    Use 'silhouette' as a fast unsupervised heuristic. For paper-faithful selection,\n",
    "    set select_by='none' and choose k later via HR/ARHR while training rGLSVD.\n",
    "\n",
    "    Args:\n",
    "        utility_matrix: (num_users, num_items) user–item matrix (binarized or not).\n",
    "        k_values: Candidate cluster counts.\n",
    "        random_state: Random seed.\n",
    "        select_by: 'silhouette' to pick the best k now; 'none' to just compute and return scores.\n",
    "\n",
    "    Returns:\n",
    "        best_labels: Cluster assignments for the selected k (or the first k if select_by='none').\n",
    "        best_k: Selected k (or the first k in k_values).\n",
    "        scores: Mapping k -> silhouette score (NaN if degenerate).\n",
    "    \"\"\"\n",
    "    scores: dict[int, float] = {}\n",
    "    label_cache: dict[int, np.ndarray] = {}\n",
    "\n",
    "    # Simple grid-search to find the best k among the tested values\n",
    "    for k in k_values:\n",
    "        labels = KMeans(\n",
    "            n_clusters=k, random_state=random_state, n_init=\"auto\"\n",
    "        ).fit_predict(utility_matrix)\n",
    "        label_cache[k] = labels\n",
    "        scores[k] = (\n",
    "            np.nan\n",
    "            if len(np.unique(labels)) == 1\n",
    "            else float(silhouette_score(utility_matrix, labels))\n",
    "        )\n",
    "\n",
    "    if select_by == \"silhouette\":\n",
    "        valid = {k: s for k, s in scores.items() if not np.isnan(s)}\n",
    "        if not valid:\n",
    "            raise ValueError(\n",
    "                \"Clustering degenerated for all k; cannot select by silhouette.\"\n",
    "            )\n",
    "        best_k = max(valid, key=valid.get)\n",
    "    else:\n",
    "        best_k = k_values[0]\n",
    "\n",
    "    return label_cache[best_k], best_k, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We follow the k subset values tested in the original paper\n",
    "k_values = [2, 3, 5, 10, 15, 20, 25, 30, 40, 50, 75, 100]\n",
    "\n",
    "best_labels, best_k, scores = cluster_users_kmeans_grid(binarized_train_u, k_values)\n",
    "\n",
    "print(f\"Best labels:\\n{best_labels}\\nBest k:\\n{best_k}\\nScores: {scores}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count number of users in each cluster\n",
    "unique, counts = np.unique(best_labels, return_counts=True)\n",
    "cluster_sizes = dict(zip(unique, counts))\n",
    "print(\"Cluster sizes:\", cluster_sizes)\n",
    "\n",
    "# Percentage distribution\n",
    "total_users = len(best_labels)\n",
    "for cluster_id, size in cluster_sizes.items():\n",
    "    print(f\"Cluster {cluster_id}: {size} users ({size / total_users:.2%})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.bar(cluster_sizes.keys(), cluster_sizes.values())\n",
    "plt.xlabel(\"Cluster ID\")\n",
    "plt.ylabel(\"Number of Users\")\n",
    "plt.title(f\"Cluster size distribution (k={best_k})\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mmd_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
